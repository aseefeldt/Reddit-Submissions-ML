{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d98d8532-4758-485a-9036-716139abade1",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c15f418b-8ac7-4970-ac55-8e6394302cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987fbaf3-2727-477c-9066-c88a5b82aa7b",
   "metadata": {},
   "source": [
    "## Reading in Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2992db21-ddfd-415a-877a-fa99f11a33b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../datasets/subs2.csv', encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ccac7035-8e78-4ad8-b95d-37b79e92f999",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>self_text</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>text_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Looking for Costa Rican Spanish speakers for a...</td>\n",
       "      <td>hey guys! i'm a linguistics phd student and i'...</td>\n",
       "      <td>0</td>\n",
       "      <td>123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Traveling to CR as a tica...</td>\n",
       "      <td>hi! i am costa_rican-american but have never b...</td>\n",
       "      <td>0</td>\n",
       "      <td>167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Puerto Viejo</td>\n",
       "      <td>sunset pic on the caribbean coast, thanks to s...</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bought a bed. They made it right in front of me!</td>\n",
       "      <td>nice craftsmanship and impressed they built an...</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How can I give back to Costa Rica?</td>\n",
       "      <td>hi, if i move to costa_rica part time as an am...</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Looking for Costa Rican Spanish speakers for a...   \n",
       "1                       Traveling to CR as a tica...   \n",
       "2                                       Puerto Viejo   \n",
       "3   Bought a bed. They made it right in front of me!   \n",
       "4                 How can I give back to Costa Rica?   \n",
       "\n",
       "                                           self_text  subreddit  text_words  \n",
       "0  hey guys! i'm a linguistics phd student and i'...          0         123  \n",
       "1  hi! i am costa_rican-american but have never b...          0         167  \n",
       "2  sunset pic on the caribbean coast, thanks to s...          0          11  \n",
       "3  nice craftsmanship and impressed they built an...          0          26  \n",
       "4  hi, if i move to costa_rica part time as an am...          0          36  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8989fe9b-ea58-44c5-a93b-21265131d0d6",
   "metadata": {},
   "source": [
    "## Reading in Custom Stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "08de2ef4-4285-425b-ad5d-f4dc060e5649",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_df = pd.read_csv('../datasets/custom_stopwords.csv')\n",
    "stopwords_new = stopwords_df['custom_stopwords'].tolist()\n",
    "stopwords = stopwords.words('english')\n",
    "stopwords_custom = stopwords +stopwords_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e5c371-0265-46d3-b6bd-0f9edbea5691",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Setting up Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0f196614-c9ff-4a5c-96e8-8d6c4708878d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X= df['self_text']\n",
    "y=df['subreddit']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8868e75f-cfce-4785-8498-1a4050231469",
   "metadata": {},
   "source": [
    "## Train Test Split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "cd1a11c9-b01b-4295-a2ba-d146f4d268d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841f22ce-1063-4095-a6d6-72e700ee5634",
   "metadata": {},
   "source": [
    "## Setting up stemming and lemmatizing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8f8eb810-eff6-409e-a1bb-1523e77b4cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(words):\n",
    "    split_words = words.split()\n",
    "    lem = WordNetLemmatizer()\n",
    "    return ' '.join([lem.lemmatize(word) for word in split_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9f2c8729-ed11-426b-8a38-f3c1504992d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(words):\n",
    "    split_words = words.split()\n",
    "    stem = PorterStemmer()\n",
    "    return ' '.join([stem.stem(word) for word in split_words])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8fac0a-53ee-4749-9a96-b0be5742bd96",
   "metadata": {},
   "source": [
    "## Gridsearch with KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7626f708-ceaa-4ac1-82b1-e3871a26f392",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    ('cvec', CountVectorizer()),\n",
    "    ('knn', KNeighborsClassifier())\n",
    "     ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8bc594db-1231-422d-a672-023b780ad230",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['doe', 'http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['doe', 'http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['doe', 'http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['doe', 'http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['doe', 'http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['doe', 'http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['doe', 'http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['doe', 'http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['doe', 'http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['doe', 'http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['doe', 'http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['doe', 'http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['doe', 'http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['doe', 'http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['doe', 'http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['doe', 'http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['doe', 'http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['doe', 'http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['doe', 'http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['doe', 'http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'anyon', 'becaus', 'befor', 'doe', 'dure', 'http', 'onc', 'onli', 'ourselv', 'thank', 'themselv', 'thi', 'veri', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'anyon', 'becaus', 'befor', 'doe', 'dure', 'http', 'onc', 'onli', 'ourselv', 'thank', 'themselv', 'thi', 'veri', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'anyon', 'becaus', 'befor', 'doe', 'dure', 'http', 'onc', 'onli', 'ourselv', 'thank', 'themselv', 'thi', 'veri', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'anyon', 'becaus', 'befor', 'doe', 'dure', 'http', 'onc', 'onli', 'ourselv', 'thank', 'themselv', 'thi', 'veri', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'anyon', 'becaus', 'befor', 'doe', 'dure', 'http', 'onc', 'onli', 'ourselv', 'thank', 'themselv', 'thi', 'veri', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'anyon', 'becaus', 'befor', 'doe', 'dure', 'http', 'onc', 'onli', 'ourselv', 'thank', 'themselv', 'thi', 'veri', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'anyon', 'becaus', 'befor', 'doe', 'dure', 'http', 'onc', 'onli', 'ourselv', 'thank', 'themselv', 'thi', 'veri', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'anyon', 'becaus', 'befor', 'doe', 'dure', 'http', 'onc', 'onli', 'ourselv', 'thank', 'themselv', 'thi', 'veri', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'anyon', 'becaus', 'befor', 'doe', 'dure', 'http', 'onc', 'onli', 'ourselv', 'thank', 'themselv', 'thi', 'veri', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'anyon', 'becaus', 'befor', 'doe', 'dure', 'http', 'onc', 'onli', 'ourselv', 'thank', 'themselv', 'thi', 'veri', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'anyon', 'becaus', 'befor', 'doe', 'dure', 'http', 'onc', 'onli', 'ourselv', 'thank', 'themselv', 'thi', 'veri', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'anyon', 'becaus', 'befor', 'doe', 'dure', 'http', 'onc', 'onli', 'ourselv', 'thank', 'themselv', 'thi', 'veri', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'anyon', 'becaus', 'befor', 'doe', 'dure', 'http', 'onc', 'onli', 'ourselv', 'thank', 'themselv', 'thi', 'veri', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'anyon', 'becaus', 'befor', 'doe', 'dure', 'http', 'onc', 'onli', 'ourselv', 'thank', 'themselv', 'thi', 'veri', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'anyon', 'becaus', 'befor', 'doe', 'dure', 'http', 'onc', 'onli', 'ourselv', 'thank', 'themselv', 'thi', 'veri', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'anyon', 'becaus', 'befor', 'doe', 'dure', 'http', 'onc', 'onli', 'ourselv', 'thank', 'themselv', 'thi', 'veri', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'anyon', 'becaus', 'befor', 'doe', 'dure', 'http', 'onc', 'onli', 'ourselv', 'thank', 'themselv', 'thi', 'veri', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'anyon', 'becaus', 'befor', 'doe', 'dure', 'http', 'onc', 'onli', 'ourselv', 'thank', 'themselv', 'thi', 'veri', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'anyon', 'becaus', 'befor', 'doe', 'dure', 'http', 'onc', 'onli', 'ourselv', 'thank', 'themselv', 'thi', 'veri', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'anyon', 'becaus', 'befor', 'doe', 'dure', 'http', 'onc', 'onli', 'ourselv', 'thank', 'themselv', 'thi', 'veri', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['anyon', 'http', 'thank'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['anyon', 'http', 'thank'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['anyon', 'http', 'thank'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['anyon', 'http', 'thank'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['anyon', 'http', 'thank'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['anyon', 'http', 'thank'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['anyon', 'http', 'thank'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['anyon', 'http', 'thank'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['anyon', 'http', 'thank'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['anyon', 'http', 'thank'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['anyon', 'http', 'thank'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['anyon', 'http', 'thank'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['anyon', 'http', 'thank'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['anyon', 'http', 'thank'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['anyon', 'http', 'thank'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['anyon', 'http', 'thank'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['anyon', 'http', 'thank'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['anyon', 'http', 'thank'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['anyon', 'http', 'thank'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['anyon', 'http', 'thank'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['doe', 'http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['doe', 'http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['doe', 'http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['doe', 'http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['doe', 'http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['doe', 'http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['doe', 'http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['doe', 'http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['doe', 'http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['doe', 'http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['doe', 'http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['doe', 'http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['doe', 'http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['doe', 'http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['doe', 'http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['doe', 'http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['doe', 'http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['doe', 'http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['doe', 'http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['doe', 'http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'anyon', 'becaus', 'befor', 'doe', 'dure', 'http', 'onc', 'onli', 'ourselv', 'thank', 'themselv', 'thi', 'veri', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'anyon', 'becaus', 'befor', 'doe', 'dure', 'http', 'onc', 'onli', 'ourselv', 'thank', 'themselv', 'thi', 'veri', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'anyon', 'becaus', 'befor', 'doe', 'dure', 'http', 'onc', 'onli', 'ourselv', 'thank', 'themselv', 'thi', 'veri', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'anyon', 'becaus', 'befor', 'doe', 'dure', 'http', 'onc', 'onli', 'ourselv', 'thank', 'themselv', 'thi', 'veri', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'anyon', 'becaus', 'befor', 'doe', 'dure', 'http', 'onc', 'onli', 'ourselv', 'thank', 'themselv', 'thi', 'veri', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'anyon', 'becaus', 'befor', 'doe', 'dure', 'http', 'onc', 'onli', 'ourselv', 'thank', 'themselv', 'thi', 'veri', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'anyon', 'becaus', 'befor', 'doe', 'dure', 'http', 'onc', 'onli', 'ourselv', 'thank', 'themselv', 'thi', 'veri', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'anyon', 'becaus', 'befor', 'doe', 'dure', 'http', 'onc', 'onli', 'ourselv', 'thank', 'themselv', 'thi', 'veri', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'anyon', 'becaus', 'befor', 'doe', 'dure', 'http', 'onc', 'onli', 'ourselv', 'thank', 'themselv', 'thi', 'veri', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'anyon', 'becaus', 'befor', 'doe', 'dure', 'http', 'onc', 'onli', 'ourselv', 'thank', 'themselv', 'thi', 'veri', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'anyon', 'becaus', 'befor', 'doe', 'dure', 'http', 'onc', 'onli', 'ourselv', 'thank', 'themselv', 'thi', 'veri', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'anyon', 'becaus', 'befor', 'doe', 'dure', 'http', 'onc', 'onli', 'ourselv', 'thank', 'themselv', 'thi', 'veri', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'anyon', 'becaus', 'befor', 'doe', 'dure', 'http', 'onc', 'onli', 'ourselv', 'thank', 'themselv', 'thi', 'veri', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'anyon', 'becaus', 'befor', 'doe', 'dure', 'http', 'onc', 'onli', 'ourselv', 'thank', 'themselv', 'thi', 'veri', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'anyon', 'becaus', 'befor', 'doe', 'dure', 'http', 'onc', 'onli', 'ourselv', 'thank', 'themselv', 'thi', 'veri', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'anyon', 'becaus', 'befor', 'doe', 'dure', 'http', 'onc', 'onli', 'ourselv', 'thank', 'themselv', 'thi', 'veri', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'anyon', 'becaus', 'befor', 'doe', 'dure', 'http', 'onc', 'onli', 'ourselv', 'thank', 'themselv', 'thi', 'veri', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'anyon', 'becaus', 'befor', 'doe', 'dure', 'http', 'onc', 'onli', 'ourselv', 'thank', 'themselv', 'thi', 'veri', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'anyon', 'becaus', 'befor', 'doe', 'dure', 'http', 'onc', 'onli', 'ourselv', 'thank', 'themselv', 'thi', 'veri', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'anyon', 'becaus', 'befor', 'doe', 'dure', 'http', 'onc', 'onli', 'ourselv', 'thank', 'themselv', 'thi', 'veri', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['anyon', 'http', 'thank'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['anyon', 'http', 'thank'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['anyon', 'http', 'thank'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['anyon', 'http', 'thank'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['anyon', 'http', 'thank'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['anyon', 'http', 'thank'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['anyon', 'http', 'thank'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['anyon', 'http', 'thank'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['anyon', 'http', 'thank'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['anyon', 'http', 'thank'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['anyon', 'http', 'thank'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['anyon', 'http', 'thank'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['anyon', 'http', 'thank'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['anyon', 'http', 'thank'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['anyon', 'http', 'thank'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['anyon', 'http', 'thank'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['anyon', 'http', 'thank'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['anyon', 'http', 'thank'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['anyon', 'http', 'thank'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['anyon', 'http', 'thank'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['doe', 'http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['doe', 'http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['doe', 'http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['doe', 'http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['doe', 'http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['doe', 'http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['doe', 'http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['doe', 'http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['doe', 'http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['doe', 'http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['doe', 'http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['doe', 'http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['doe', 'http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['doe', 'http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['doe', 'http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['doe', 'http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['doe', 'http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['doe', 'http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['doe', 'http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['doe', 'http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'anyon', 'becaus', 'befor', 'doe', 'dure', 'http', 'onc', 'onli', 'ourselv', 'thank', 'themselv', 'thi', 'veri', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'anyon', 'becaus', 'befor', 'doe', 'dure', 'http', 'onc', 'onli', 'ourselv', 'thank', 'themselv', 'thi', 'veri', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'anyon', 'becaus', 'befor', 'doe', 'dure', 'http', 'onc', 'onli', 'ourselv', 'thank', 'themselv', 'thi', 'veri', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'anyon', 'becaus', 'befor', 'doe', 'dure', 'http', 'onc', 'onli', 'ourselv', 'thank', 'themselv', 'thi', 'veri', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'anyon', 'becaus', 'befor', 'doe', 'dure', 'http', 'onc', 'onli', 'ourselv', 'thank', 'themselv', 'thi', 'veri', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'anyon', 'becaus', 'befor', 'doe', 'dure', 'http', 'onc', 'onli', 'ourselv', 'thank', 'themselv', 'thi', 'veri', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'anyon', 'becaus', 'befor', 'doe', 'dure', 'http', 'onc', 'onli', 'ourselv', 'thank', 'themselv', 'thi', 'veri', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'anyon', 'becaus', 'befor', 'doe', 'dure', 'http', 'onc', 'onli', 'ourselv', 'thank', 'themselv', 'thi', 'veri', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'anyon', 'becaus', 'befor', 'doe', 'dure', 'http', 'onc', 'onli', 'ourselv', 'thank', 'themselv', 'thi', 'veri', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'anyon', 'becaus', 'befor', 'doe', 'dure', 'http', 'onc', 'onli', 'ourselv', 'thank', 'themselv', 'thi', 'veri', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'anyon', 'becaus', 'befor', 'doe', 'dure', 'http', 'onc', 'onli', 'ourselv', 'thank', 'themselv', 'thi', 'veri', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'anyon', 'becaus', 'befor', 'doe', 'dure', 'http', 'onc', 'onli', 'ourselv', 'thank', 'themselv', 'thi', 'veri', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'anyon', 'becaus', 'befor', 'doe', 'dure', 'http', 'onc', 'onli', 'ourselv', 'thank', 'themselv', 'thi', 'veri', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'anyon', 'becaus', 'befor', 'doe', 'dure', 'http', 'onc', 'onli', 'ourselv', 'thank', 'themselv', 'thi', 'veri', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'anyon', 'becaus', 'befor', 'doe', 'dure', 'http', 'onc', 'onli', 'ourselv', 'thank', 'themselv', 'thi', 'veri', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'anyon', 'becaus', 'befor', 'doe', 'dure', 'http', 'onc', 'onli', 'ourselv', 'thank', 'themselv', 'thi', 'veri', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'anyon', 'becaus', 'befor', 'doe', 'dure', 'http', 'onc', 'onli', 'ourselv', 'thank', 'themselv', 'thi', 'veri', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'anyon', 'becaus', 'befor', 'doe', 'dure', 'http', 'onc', 'onli', 'ourselv', 'thank', 'themselv', 'thi', 'veri', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'anyon', 'becaus', 'befor', 'doe', 'dure', 'http', 'onc', 'onli', 'ourselv', 'thank', 'themselv', 'thi', 'veri', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'anyon', 'becaus', 'befor', 'doe', 'dure', 'http', 'onc', 'onli', 'ourselv', 'thank', 'themselv', 'thi', 'veri', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['anyon', 'http', 'thank'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['anyon', 'http', 'thank'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['anyon', 'http', 'thank'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['anyon', 'http', 'thank'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['anyon', 'http', 'thank'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['anyon', 'http', 'thank'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['anyon', 'http', 'thank'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['anyon', 'http', 'thank'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['anyon', 'http', 'thank'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['anyon', 'http', 'thank'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['anyon', 'http', 'thank'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['anyon', 'http', 'thank'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['anyon', 'http', 'thank'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['anyon', 'http', 'thank'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['anyon', 'http', 'thank'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['anyon', 'http', 'thank'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['anyon', 'http', 'thank'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['anyon', 'http', 'thank'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['anyon', 'http', 'thank'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['anyon', 'http', 'thank'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['doe', 'http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['doe', 'http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['doe', 'http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['doe', 'http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['doe', 'http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['doe', 'http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['doe', 'http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['doe', 'http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['doe', 'http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['doe', 'http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['doe', 'http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['doe', 'http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['doe', 'http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['doe', 'http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['doe', 'http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['doe', 'http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['doe', 'http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['doe', 'http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['doe', 'http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['doe', 'http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['http'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'anyon', 'becaus', 'befor', 'doe', 'dure', 'http', 'onc', 'onli', 'ourselv', 'thank', 'themselv', 'thi', 'veri', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'anyon', 'becaus', 'befor', 'doe', 'dure', 'http', 'onc', 'onli', 'ourselv', 'thank', 'themselv', 'thi', 'veri', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'anyon', 'becaus', 'befor', 'doe', 'dure', 'http', 'onc', 'onli', 'ourselv', 'thank', 'themselv', 'thi', 'veri', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'anyon', 'becaus', 'befor', 'doe', 'dure', 'http', 'onc', 'onli', 'ourselv', 'thank', 'themselv', 'thi', 'veri', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'anyon', 'becaus', 'befor', 'doe', 'dure', 'http', 'onc', 'onli', 'ourselv', 'thank', 'themselv', 'thi', 'veri', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'anyon', 'becaus', 'befor', 'doe', 'dure', 'http', 'onc', 'onli', 'ourselv', 'thank', 'themselv', 'thi', 'veri', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'anyon', 'becaus', 'befor', 'doe', 'dure', 'http', 'onc', 'onli', 'ourselv', 'thank', 'themselv', 'thi', 'veri', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'anyon', 'becaus', 'befor', 'doe', 'dure', 'http', 'onc', 'onli', 'ourselv', 'thank', 'themselv', 'thi', 'veri', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'anyon', 'becaus', 'befor', 'doe', 'dure', 'http', 'onc', 'onli', 'ourselv', 'thank', 'themselv', 'thi', 'veri', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'anyon', 'becaus', 'befor', 'doe', 'dure', 'http', 'onc', 'onli', 'ourselv', 'thank', 'themselv', 'thi', 'veri', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'anyon', 'becaus', 'befor', 'doe', 'dure', 'http', 'onc', 'onli', 'ourselv', 'thank', 'themselv', 'thi', 'veri', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'anyon', 'becaus', 'befor', 'doe', 'dure', 'http', 'onc', 'onli', 'ourselv', 'thank', 'themselv', 'thi', 'veri', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'anyon', 'becaus', 'befor', 'doe', 'dure', 'http', 'onc', 'onli', 'ourselv', 'thank', 'themselv', 'thi', 'veri', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'anyon', 'becaus', 'befor', 'doe', 'dure', 'http', 'onc', 'onli', 'ourselv', 'thank', 'themselv', 'thi', 'veri', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'anyon', 'becaus', 'befor', 'doe', 'dure', 'http', 'onc', 'onli', 'ourselv', 'thank', 'themselv', 'thi', 'veri', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'anyon', 'becaus', 'befor', 'doe', 'dure', 'http', 'onc', 'onli', 'ourselv', 'thank', 'themselv', 'thi', 'veri', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'anyon', 'becaus', 'befor', 'doe', 'dure', 'http', 'onc', 'onli', 'ourselv', 'thank', 'themselv', 'thi', 'veri', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'anyon', 'becaus', 'befor', 'doe', 'dure', 'http', 'onc', 'onli', 'ourselv', 'thank', 'themselv', 'thi', 'veri', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'anyon', 'becaus', 'befor', 'doe', 'dure', 'http', 'onc', 'onli', 'ourselv', 'thank', 'themselv', 'thi', 'veri', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'anyon', 'becaus', 'befor', 'doe', 'dure', 'http', 'onc', 'onli', 'ourselv', 'thank', 'themselv', 'thi', 'veri', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['anyon', 'http', 'thank'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['anyon', 'http', 'thank'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['anyon', 'http', 'thank'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['anyon', 'http', 'thank'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['anyon', 'http', 'thank'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['anyon', 'http', 'thank'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['anyon', 'http', 'thank'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['anyon', 'http', 'thank'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['anyon', 'http', 'thank'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['anyon', 'http', 'thank'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['anyon', 'http', 'thank'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['anyon', 'http', 'thank'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['anyon', 'http', 'thank'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['anyon', 'http', 'thank'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['anyon', 'http', 'thank'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['anyon', 'http', 'thank'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['anyon', 'http', 'thank'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['anyon', 'http', 'thank'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['anyon', 'http', 'thank'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['anyon', 'http', 'thank'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\seefe\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['anyon', 'http', 'thank'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(estimator=Pipeline(steps=[(&#x27;cvec&#x27;, CountVectorizer()),\n",
       "                                       (&#x27;knn&#x27;, KNeighborsClassifier())]),\n",
       "             param_grid={&#x27;cvec__ngram_range&#x27;: [(1, 3), (1, 1), (1, 2), (2, 2)],\n",
       "                         &#x27;cvec__preprocessor&#x27;: [None,\n",
       "                                                &lt;function lemmatize at 0x0000023198C95E10&gt;,\n",
       "                                                &lt;function stem at 0x0000023198C96050&gt;],\n",
       "                         &#x27;cvec__stop_words&#x27;: [None, &#x27;english&#x27;,\n",
       "                                              [&#x27;i&#x27;, &#x27;me&#x27;, &#x27;my&#x27;, &#x27;myself&#x27;, &#x27;we&#x27;,\n",
       "                                               &#x27;our&#x27;, &#x27;ours&#x27;, &#x27;ourselves&#x27;,\n",
       "                                               &#x27;...\n",
       "                                               &quot;you&#x27;ll&quot;, &quot;you&#x27;d&quot;, &#x27;your&#x27;,\n",
       "                                               &#x27;yours&#x27;, &#x27;yourself&#x27;,\n",
       "                                               &#x27;yourselves&#x27;, &#x27;he&#x27;, &#x27;him&#x27;, &#x27;his&#x27;,\n",
       "                                               &#x27;himself&#x27;, &#x27;she&#x27;, &quot;she&#x27;s&quot;, &#x27;her&#x27;,\n",
       "                                               &#x27;hers&#x27;, &#x27;herself&#x27;, &#x27;it&#x27;, &quot;it&#x27;s&quot;,\n",
       "                                               &#x27;its&#x27;, &#x27;itself&#x27;, ...],\n",
       "                                              [&#x27;thanks&#x27;, &#x27;good&#x27;, &#x27;know&#x27;, &#x27;want&#x27;,\n",
       "                                               &#x27;time&#x27;, &#x27;just&#x27;, &#x27;like&#x27;, &#x27;day&#x27;,\n",
       "                                               &#x27;ve&#x27;, &#x27;wa&#x27;, &#x27;don&#x27;, &#x27;de&#x27;,\n",
       "                                               &#x27;thanks&#x27;, &#x27;que&#x27;, &#x27;hi&#x27;, &#x27;hey&#x27;,\n",
       "                                               &#x27;anyone&#x27;, &#x27;us&#x27;, &#x27;get&#x27;, &#x27;also&#x27;,\n",
       "                                               &#x27;https&#x27;, &#x27;ha&#x27;, &#x27;le&#x27;, &#x27;un&#x27;, &#x27;el&#x27;,\n",
       "                                               &#x27;la&#x27;, &#x27;en&#x27;, &#x27;es&#x27;]],\n",
       "                         &#x27;knn__n_neighbors&#x27;: [3, 5, 7, 10]})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(estimator=Pipeline(steps=[(&#x27;cvec&#x27;, CountVectorizer()),\n",
       "                                       (&#x27;knn&#x27;, KNeighborsClassifier())]),\n",
       "             param_grid={&#x27;cvec__ngram_range&#x27;: [(1, 3), (1, 1), (1, 2), (2, 2)],\n",
       "                         &#x27;cvec__preprocessor&#x27;: [None,\n",
       "                                                &lt;function lemmatize at 0x0000023198C95E10&gt;,\n",
       "                                                &lt;function stem at 0x0000023198C96050&gt;],\n",
       "                         &#x27;cvec__stop_words&#x27;: [None, &#x27;english&#x27;,\n",
       "                                              [&#x27;i&#x27;, &#x27;me&#x27;, &#x27;my&#x27;, &#x27;myself&#x27;, &#x27;we&#x27;,\n",
       "                                               &#x27;our&#x27;, &#x27;ours&#x27;, &#x27;ourselves&#x27;,\n",
       "                                               &#x27;...\n",
       "                                               &quot;you&#x27;ll&quot;, &quot;you&#x27;d&quot;, &#x27;your&#x27;,\n",
       "                                               &#x27;yours&#x27;, &#x27;yourself&#x27;,\n",
       "                                               &#x27;yourselves&#x27;, &#x27;he&#x27;, &#x27;him&#x27;, &#x27;his&#x27;,\n",
       "                                               &#x27;himself&#x27;, &#x27;she&#x27;, &quot;she&#x27;s&quot;, &#x27;her&#x27;,\n",
       "                                               &#x27;hers&#x27;, &#x27;herself&#x27;, &#x27;it&#x27;, &quot;it&#x27;s&quot;,\n",
       "                                               &#x27;its&#x27;, &#x27;itself&#x27;, ...],\n",
       "                                              [&#x27;thanks&#x27;, &#x27;good&#x27;, &#x27;know&#x27;, &#x27;want&#x27;,\n",
       "                                               &#x27;time&#x27;, &#x27;just&#x27;, &#x27;like&#x27;, &#x27;day&#x27;,\n",
       "                                               &#x27;ve&#x27;, &#x27;wa&#x27;, &#x27;don&#x27;, &#x27;de&#x27;,\n",
       "                                               &#x27;thanks&#x27;, &#x27;que&#x27;, &#x27;hi&#x27;, &#x27;hey&#x27;,\n",
       "                                               &#x27;anyone&#x27;, &#x27;us&#x27;, &#x27;get&#x27;, &#x27;also&#x27;,\n",
       "                                               &#x27;https&#x27;, &#x27;ha&#x27;, &#x27;le&#x27;, &#x27;un&#x27;, &#x27;el&#x27;,\n",
       "                                               &#x27;la&#x27;, &#x27;en&#x27;, &#x27;es&#x27;]],\n",
       "                         &#x27;knn__n_neighbors&#x27;: [3, 5, 7, 10]})</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" ><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;cvec&#x27;, CountVectorizer()), (&#x27;knn&#x27;, KNeighborsClassifier())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" ><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-12\" type=\"checkbox\" ><label for=\"sk-estimator-id-12\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KNeighborsClassifier</label><div class=\"sk-toggleable__content\"><pre>KNeighborsClassifier()</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(estimator=Pipeline(steps=[('cvec', CountVectorizer()),\n",
       "                                       ('knn', KNeighborsClassifier())]),\n",
       "             param_grid={'cvec__ngram_range': [(1, 3), (1, 1), (1, 2), (2, 2)],\n",
       "                         'cvec__preprocessor': [None,\n",
       "                                                <function lemmatize at 0x0000023198C95E10>,\n",
       "                                                <function stem at 0x0000023198C96050>],\n",
       "                         'cvec__stop_words': [None, 'english',\n",
       "                                              ['i', 'me', 'my', 'myself', 'we',\n",
       "                                               'our', 'ours', 'ourselves',\n",
       "                                               '...\n",
       "                                               \"you'll\", \"you'd\", 'your',\n",
       "                                               'yours', 'yourself',\n",
       "                                               'yourselves', 'he', 'him', 'his',\n",
       "                                               'himself', 'she', \"she's\", 'her',\n",
       "                                               'hers', 'herself', 'it', \"it's\",\n",
       "                                               'its', 'itself', ...],\n",
       "                                              ['thanks', 'good', 'know', 'want',\n",
       "                                               'time', 'just', 'like', 'day',\n",
       "                                               've', 'wa', 'don', 'de',\n",
       "                                               'thanks', 'que', 'hi', 'hey',\n",
       "                                               'anyone', 'us', 'get', 'also',\n",
       "                                               'https', 'ha', 'le', 'un', 'el',\n",
       "                                               'la', 'en', 'es']],\n",
       "                         'knn__n_neighbors': [3, 5, 7, 10]})"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {\n",
    "    'cvec__stop_words' : [None, \"english\", stopwords_custom, stopwords_new],\n",
    "    'cvec__preprocessor': [None, lemmatize, stem],\n",
    "    'cvec__ngram_range': [(1,3), (1,1), (1,2), (2,2)],\n",
    "    'knn__n_neighbors': [3, 5, 7, 10] \n",
    "}\n",
    "\n",
    "gs = GridSearchCV(pipe, param_grid=params)\n",
    "gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "7880e879-6a37-461d-b55b-4c7a66b18230",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.7089201877934272\n",
      "Test: 0.5762910798122066\n"
     ]
    }
   ],
   "source": [
    "print('Train:', gs.score(X_train, y_train))\n",
    "print('Test:', gs.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3e9c4bab-710c-4031-b595-bc8f93e78e34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cvec__ngram_range': (1, 1),\n",
       " 'cvec__preprocessor': <function __main__.stem(words)>,\n",
       " 'cvec__stop_words': ['thanks',\n",
       "  'good',\n",
       "  'know',\n",
       "  'want',\n",
       "  'time',\n",
       "  'just',\n",
       "  'like',\n",
       "  'day',\n",
       "  've',\n",
       "  'wa',\n",
       "  'don',\n",
       "  'de',\n",
       "  'thanks',\n",
       "  'que',\n",
       "  'hi',\n",
       "  'hey',\n",
       "  'anyone',\n",
       "  'us',\n",
       "  'get',\n",
       "  'also',\n",
       "  'https',\n",
       "  'ha',\n",
       "  'le',\n",
       "  'un',\n",
       "  'el',\n",
       "  'la',\n",
       "  'en',\n",
       "  'es'],\n",
       " 'knn__n_neighbors': 7}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "082b2a27-2d45-40cc-b7d5-041e78a981e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preds = gs.predict(X_test)\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, preds, cmap=plt.cm.Oranges)\n",
    "plt.title(\"Confusion Maxtrix with KNN\");\n",
    "plt.savefig('../pictures/Confusion Maxtrix with KNN.png', format='png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
